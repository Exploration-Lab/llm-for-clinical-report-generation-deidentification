{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Count summaries\n",
    "# directory_path = \"/lockbox/llama3_20240509/llama3/\"\n",
    "# text_file_count = len([filename for filename in os.listdir(directory_path) if filename.endswith('.txt')])\n",
    "# text_file_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_folders = '/lockbox/sgpgi_ds/'\n",
    "path_to_summaries = \"prompt2_pii_detection/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_directory_if_not_exists(directory_path):\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "        print(f\"Directory created at {directory_path}\")\n",
    "    else:\n",
    "        print(f\"Directory already exists at {directory_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# def summary_to_xml(json_data):\n",
    "\n",
    "#     text_element = ''\n",
    "#     text = json_data[\"text\"]\n",
    "#     ner_entities = json_data[\"entities\"]\n",
    "\n",
    "#     chunks = []\n",
    "#     start_index = 0\n",
    "#     for entity in ner_entities:\n",
    "#         end_index = text.find(entity[\"fake_entity\"], start_index)\n",
    "#         if end_index != -1:\n",
    "#             chunks.append(text[start_index:end_index])\n",
    "#             chunks.append(entity)\n",
    "#             start_index = end_index + len(entity[\"fake_entity\"])\n",
    "#     chunks.append(text[start_index:])\n",
    "\n",
    "#     template = '<ENTITY TYPE=\"{}\">{}</ENTITY>'\n",
    "\n",
    "#     # Build XML structure\n",
    "#     for chunk in chunks:\n",
    "#         if isinstance(chunk, dict):\n",
    "#             text_element += template.format(chunk[\"label\"], chunk[\"fake_entity\"])\n",
    "#         else:\n",
    "#             text_element += chunk\n",
    "\n",
    "#     return text_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgpgi_ds = []\n",
    "folder_path_name = []\n",
    "\n",
    "# Iterate through each folder in the specified path\n",
    "for folder in os.listdir(path_to_folders):#\n",
    "    \n",
    "    folder_path = os.path.join(path_to_folders, folder)\n",
    "\n",
    "    # Check if the item in the folder is a directory\n",
    "    if os.path.isdir(folder_path) and int(folder_path.split('/')[-1]) in range(80,100):\n",
    "        \n",
    "        folder_path_name.append(folder_path)\n",
    "        # Iterate through each file in the directory\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            \n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            # Check if the file is a JSONL file\n",
    "            if file_name.endswith('.jsonl'):\n",
    "                # Open the JSONL file and read each line\n",
    "                with open(file_path, 'r') as json_file:\n",
    "                    all_text = []\n",
    "                    for index,line in enumerate(json_file):\n",
    "                        data = json.loads(line)\n",
    "                        # Check if 'text' key exists in the JSON data\n",
    "                        # if 'text' in data:\n",
    "                        all_text.append(data[\"text\"])\n",
    "                         \n",
    "                    sgpgi_ds.append(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sgpgi_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT =\"\"\"You are an expert in annotating the entities given in the prompt.\n",
    "Make sure to return annotated summaries without altering/rephrasing the text in the original discharge summary.\n",
    "Your task is to annotate the given discharge summary with only the given list of annotations in the prompt.\n",
    "\n",
    "List of annotations:\n",
    "- Patient_Name\n",
    "- Hospital_Name\n",
    "- Staff_Name\n",
    "- Doctor_Name\n",
    "- Guardian_Name\n",
    "- Age\n",
    "- Patient_ID\n",
    "- Patient_DOB\n",
    "- Treatment_Date\n",
    "- Phone_No\n",
    "- City\n",
    "- State\n",
    "- Street\n",
    "- Zip\n",
    "- Country\n",
    "- Other_Location\n",
    "- Landline\n",
    "- Email\n",
    "- IP_Address\n",
    "- Fax\n",
    "- Ward_Location\n",
    "- Insurance_Number\n",
    "- Web_url\n",
    "- Aadhar\n",
    "- Driver_License\n",
    "- Voter_ID\n",
    "- PAN_Card\n",
    "\n",
    "EXAMPLE:  \n",
    "\n",
    "'annotated output' : \n",
    "<INSERT_EXAMPLE_MEDICAL_REPORT>\n",
    "\n",
    "Instructions: \n",
    "It's very crucial to provide annotations for the discharge summary text with as much accuracy and detail as possible. \n",
    "Please always enclose the annotated output with <RECORD> </RECORD> tags.\n",
    "\n",
    "Now, annotate the following discharge summary text: \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### meta-llama/Meta-Llama-3-8B-Instruct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'llama3'\n",
    "make_directory_if_not_exists(path_to_summaries + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Configure logging to write to a file\n",
    "logging.basicConfig(filename=f'{path_to_summaries}{model_name}_log.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/lockbox/models/Meta-Llama-3-8B-Instruct/\",device_map=\"cuda\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/lockbox/models/Meta-Llama-3-8B-Instruct/\",device_map=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for summary_number in range(len(folder_path_name)):\n",
    "    for i,summary in enumerate(sgpgi_ds[summary_number]):\n",
    "\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": PROMPT},\n",
    "        {\"role\": \"user\", \"content\": summary},]\n",
    "    \n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "    \n",
    "        terminators = [\n",
    "            tokenizer.eos_token_id,\n",
    "            tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            output = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=3000,\n",
    "                eos_token_id=terminators,\n",
    "                do_sample=True,\n",
    "                temperature=0.9,\n",
    "                top_p=0.9\n",
    "            )\n",
    "            response = output[0][input_ids.shape[-1]:]\n",
    "            generated_summary = tokenizer.decode(response, skip_special_tokens=True)\n",
    "    \n",
    "            file_name = f\"llama3_pii_{folder_path_name[summary_number].split('/')[-1]}_{i}.txt\"\n",
    "            \n",
    "            # file_name = f\"llama3_generated_ds_{i}.txt\"\n",
    "            file_path = f\"{path_to_summaries}{model_name}/{file_name}\"\n",
    "    \n",
    "            # Open the file in write mode and save the content\n",
    "            with open(file_path, \"w\") as file:\n",
    "                file.write(generated_summary)\n",
    "                # print('#'*100)\n",
    "                # print(generated_summary)\n",
    "                # print('*'*100)\n",
    "    \n",
    "            logging.info(f\"Content saved to {file_path}\")          \n",
    "            \n",
    "        except (ValueError, Exception) as e:\n",
    "            logging.error(f\"File No.: {folder_path_name[summary_number]} Error occurred: {e}. Moving on to the next iteration...\")\n",
    "            continue\n",
    "print('Fin!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### google/gemma-1.1-7b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gemma'\n",
    "make_directory_if_not_exists(path_to_summaries + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging to write to a file\n",
    "logging.basicConfig(filename=f'{path_to_summaries}{model_name}_log.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/lockbox/models/gemma-1.1-7b-it/\",device_map=\"cuda\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/lockbox/models/gemma-1.1-7b-it/\",device_map=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for summary_number in range(len(folder_path_name)):\n",
    "\n",
    "    for i,summary in enumerate(sgpgi_ds[summary_number]):\n",
    "\n",
    "        prompt = PROMPT.format(summary)\n",
    "\n",
    "        try:\n",
    "            chat = [\n",
    "                { \"role\": \"user\", \"content\":  prompt},\n",
    "            ]\n",
    "            prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "            inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "            outputs = model.generate(input_ids=inputs.to(model.device),  temperature = 0.9, max_length=3000) # max_length=model.config.max_position_embeddings - 2\n",
    "            decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                        \n",
    "            # Define the file path with the timestamp in the name\n",
    "            file_name = f\"gemma_generated_ds{folder_path_name[summary_number].split('/')[-1]}_{i}.txt\"\n",
    "            \n",
    "            # file_name = f\"llama3_generated_ds_{i}.txt\"\n",
    "            file_path = f\"{path_to_summaries}{model_name}/{file_name}\"\n",
    "            \n",
    "            # Open the file in write mode and save the content\n",
    "            with open(file_path, \"w\") as file:\n",
    "                file.write(decoded_output.split('model')[-1])\n",
    "\n",
    "            logging.info(f\"Content saved to {file_path}\")\n",
    "            \n",
    "        except (ValueError, Exception) as e:\n",
    "            logging.error(f\"File No.: {folder_path_name[summary_number]} Error occurred: {e}. Moving on to the next iteration...\")\n",
    "            continue   \n",
    "\n",
    "print('Fin!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mistralai/Mistral-7B-Instruct-v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mistral'\n",
    "make_directory_if_not_exists(path_to_summaries + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging to write to a file\n",
    "logging.basicConfig(filename=f'{path_to_summaries}{model_name}_log.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"/lockbox/models/Mistral-7B-Instruct-v0.1/\",device_map=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/lockbox/models/Mistral-7B-Instruct-v0.1/\",device_map=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for summary_number in range(len(folder_path_name)):\n",
    "\n",
    "    for i,summary in enumerate(sgpgi_ds[summary_number]):\n",
    "\n",
    "        prompt = PROMPT.format(summary)\n",
    "        \n",
    "        try:\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ]\n",
    "            encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "            model_inputs = encodeds.to(model.device)\n",
    "\n",
    "            generated_ids = model.generate(model_inputs, max_new_tokens=3000, do_sample=True,temperature=0.9)\n",
    "            decoded = tokenizer.batch_decode(generated_ids)\n",
    "            \n",
    "            decoded_output = decoded[0].split('[/INST]')[-1]\n",
    "            \n",
    "            # Define the file path with the timestamp in the name\n",
    "            file_name = f\"mistral_generated_ds{folder_path_name[summary_number].split('/')[-1]}_{i}.txt\"\n",
    "            \n",
    "            file_path = f\"{path_to_summaries}{model_name}/{file_name}\"\n",
    "            \n",
    "            # Open the file in write mode and save the content\n",
    "            with open(file_path, \"w\") as file:\n",
    "                file.write(decoded_output.split('model')[-1])\n",
    "\n",
    "            logging.info(f\"Content saved to {file_path}\")\n",
    "            \n",
    "        except (ValueError, Exception) as e:\n",
    "            logging.error(f\"File No.: {folder_path_name[summary_number]} Error occurred: {e}. Moving on to the next iteration...\")\n",
    "            continue   \n",
    "\n",
    "print('Fin!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_all_keys(rawText):\n",
    "    keys=[]\n",
    "    soup = BeautifulSoup(rawText, 'xml')\n",
    "    for record in soup.find_all('RECORD'):\n",
    "        text = record.text\n",
    "        start = 0\n",
    "        tag = []\n",
    "        for phi in record.find_all('ENTITY'):\n",
    "            start = text.find(phi.text, start)\n",
    "            end = start + len(phi.text)\n",
    "            try:\n",
    "                tagType = phi['TYPE']\n",
    "                keys.append(tagType)\n",
    "                tag.append({'start': str(start), 'end': str(end), 'text': phi.text, 'label': tagType})\n",
    "            except Exception:\n",
    "                pass\n",
    "    \n",
    "    return keys\n",
    "        \n",
    "\n",
    "#folder_path = \"/lockbox/summaries_pii/\"\n",
    "folder_path = \"prompt2_pii_detection/\"\n",
    "all_tags = []\n",
    "for folder in os.listdir(folder_path):\n",
    "    if not 'ipynb' in folder:\n",
    "        folder_path = os.path.join(folder_path, folder)\n",
    "\n",
    "        if  'log' not in folder_path:\n",
    "\n",
    "            for file_name in os.listdir(folder_path):\n",
    "    \n",
    "            #for file_name in os.listdir(folder_path):\n",
    "                if file_name.endswith(\".txt\") and 'log' not in file_name:\n",
    "                    file_path = os.path.join(folder_path, file_name)\n",
    "                    #print(file_path)\n",
    "                    \n",
    "                    with open(file_path, 'r') as file:\n",
    "                        file_contents = file.read()\n",
    "    \n",
    "                        all_tags.extend(get_all_keys(file_contents))\n",
    "\n",
    "\n",
    "gold_tags = ['Patient_Name','Hospital_Name','Staff_Name','Doctor_Name','Age','Gaurdian_Name','Gender','Patient_ID','Misc_Medical_ID','Aadhar',    'Driver_License','Voter_ID','PAN_Card','Patient_DOB','Treatment_Date',\n",
    "'Treatment_Time','Phone_No','Landline','Email','IP_Address','Fax',      'Doctor_Specialisation','Patient_Profession','City','Ward_Location',\n",
    "'Device_Number','Other_Info','State','Street','Zip','Country',       'Other_Location','Other_Govt_ID','Insurance_Number','Web_url']\n",
    "\n",
    "\n",
    "set(gold_tags) - set(all_tags)\n",
    "#set(all_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install nervaluate\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from nervaluate import Evaluator\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "def read_xml(text,file_path):\n",
    "    tags=[]        \n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "        pattern = r'<RECORD>(.*?)<\\/RECORD>'\n",
    "        matches = re.findall(pattern, content, re.DOTALL)\n",
    "        record = ''.join(matches)\n",
    "        start = 0\n",
    "        entity_matches = re.finditer(r'<ENTITY TYPE=\"(.*?)\">(.*?)<\\/ENTITY>', str(record), re.DOTALL)\n",
    "        \n",
    "        for match in entity_matches:\n",
    "            tag = []\n",
    "            start = text.find(match.group(2))  # Find the start index of the entity text\n",
    "            end = start + len(match.group(2))  # Calculate the end index\n",
    "            \n",
    "            tag.append({\n",
    "                'start': str(start),\n",
    "                'end': str(end),\n",
    "                'text': match.group(2),\n",
    "                'label': match.group(1)\n",
    "            })\n",
    "\n",
    "            tags.extend(tag)\n",
    "    return tags\n",
    "\n",
    "def modelfilepath(model_str, line_str):\n",
    "    directory = f'prompt2_pii_detection/{model_str}'\n",
    "    for filename in os.listdir(directory):\n",
    "        if model_str in filename and  line_str in filename:\n",
    "            return True , os.path.join(directory, filename)\n",
    "    return False,''\n",
    "    \n",
    "def get_annotations(model):\n",
    "    sgpgi_data =[]\n",
    "    model_data=[]\n",
    "    for subdir, dirs, files in os.walk('/lockbox/sgpgi_ds/'):\n",
    "        subdir_split = subdir.split('/')\n",
    "        if len(subdir_split) >= 3 and subdir_split[3] in  set(str(i) for i in range(80, 100)): #['01','02','03','04','05']:\n",
    "            for file in files:\n",
    "                if file.endswith(\".jsonl\"):\n",
    "                    file_path = os.path.join(subdir, file)\n",
    "                    with open(file_path, \"r\") as file:\n",
    "                        for index, line in enumerate(file, start=1):\n",
    "                            data = json.loads(line.strip())\n",
    "                            text = data.get(\"text\")\n",
    "                            one_doc = []\n",
    "                            exists, filepath = modelfilepath(model, f'{os.path.basename(subdir_split[3])}_{index-1}.txt')\n",
    "                            if  exists :\n",
    "                                model_data.append(read_xml(text,filepath))\n",
    "                            else:\n",
    "                                model_data.append([])\n",
    "                            for entity in data.get(\"entities\"):\n",
    "                                one_doc.append({'label': entity['label'],\n",
    "                                                       'start': entity['start_offset'],\n",
    "                                                       'end': entity['end_offset'],\n",
    "                                                       'fake_entity': entity['fake_entity']})\n",
    "                            sgpgi_data.append(one_doc)\n",
    "    return sgpgi_data,model_data\n",
    "\n",
    "labels = ['DATE','LOCATION','HOSPITAL','ID','AGE','PHONE','DOCTOR','PATIENT']\n",
    "def map_labels(annotations):\n",
    "    for item in annotations:\n",
    "        for value in item:\n",
    "            if value['label'] in ['Patient_Name','Gaurdian_Name']:\n",
    "                value['label'] = 'PATIENT'\n",
    "            elif value['label'] in ['Staff_Name','Doctor_Name']:\n",
    "                value['label'] = 'DOCTOR'\n",
    "            elif value['label'] in ['Patient_DOB','Treatment_Date']:#,'Treatment_Time'\n",
    "                value['label'] = 'DATE'\n",
    "            elif value['label'] in ['Hospital_Name','Ward_Location']:\n",
    "                value['label'] = 'HOSPITAL'\n",
    "            elif value['label'] == 'Age':\n",
    "                value['label'] = 'AGE'\n",
    "            elif value['label'] in ['Other_Govt_ID','Insurance_Number','Patient_ID','Misc_Medical_ID','Aadhar','Driver_License','Voter_ID','PAN_Card','Device_Number']:\n",
    "                value['label'] = 'ID'\n",
    "            elif value['label'] in ['Phone_No','Landline','Email','IP_Address','Fax','Web_url']:\n",
    "                value['label'] = 'CONTACT'\n",
    "            elif value['label'] in ['City','State','Street','Zip','Country', 'Other_Location',]:\n",
    "                value['label'] = 'LOCATION'\n",
    "\n",
    "            value['start'] = int(value['start'])\n",
    "            value['end'] = int(value['end'])\n",
    "\n",
    "for key in ['llama3','gemma','mistral']:\n",
    "    print(key)\n",
    "    gold_annotations,pred_annotations = get_annotations(key)\n",
    "    print(len(gold_annotations), len(pred_annotations))\n",
    "    map_labels(gold_annotations)\n",
    "    map_labels(pred_annotations)\n",
    "\n",
    "    results, results_per_tag = Evaluator(gold_annotations, pred_annotations, tags=labels).evaluate()\n",
    "\n",
    "    if not os.path.exists('results/'):\n",
    "        os.makedirs('results/')\n",
    "    with open(f'results/{key}_results.json', 'w') as results_file:\n",
    "        json.dump(results, results_file, indent=4)\n",
    "    with open(f'results/{key}_results_per_tag.json', 'w') as results_per_tag_file:\n",
    "        json.dump(results_per_tag, results_per_tag_file, indent=4)\n",
    "print('Fin!!!!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
