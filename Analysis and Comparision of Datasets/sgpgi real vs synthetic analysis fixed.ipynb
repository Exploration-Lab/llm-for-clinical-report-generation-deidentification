{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11751327-d49b-4b4f-ad06-b85600782730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160f0294-aa53-4464-b89b-afdd5bf2e3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3748c067-ad78-4ef3-b992-22716b1220e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = '/lockbox/sgpgi_ds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2243138a-dbe7-4625-89a1-558450363170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_parse_jsonl(root_directory):\n",
    "    parsed_json_data = [] \n",
    "    for subdir, _, files in os.walk(root_directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.jsonl'):\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                try:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        for line in f:\n",
    "                            try:\n",
    "                                json_data = json.loads(line)\n",
    "                                parsed_json_data.append(json_data)\n",
    "                            except json.JSONDecodeError as e:\n",
    "                                print(f\"Error parsing JSON in file '{file_path}': {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing file '{file_path}': {e}\")\n",
    "    return parsed_json_data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfbfa3c-e810-49ce-bc68-50452a642402",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = extract_and_parse_jsonl(root_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286425e4-cda3-43bb-a1fc-159ad350048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cad44d-5c08-4151-aa00-3d60a84b0d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_texts = [entry['text'] for entry in texts if 'text' in entry]\n",
    "for text in extracted_texts:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f958218-8439-4438-8d8e-5991baf02649",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(extracted_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed76f447-6290-41f0-b634-370b42f0c8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340358e0-e9db-4a1b-ac67-c2db6ba903c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb6f945-f1c1-44c8-9a87-9e39308efeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_texts = [clean_text(text) for text in extracted_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bdcbe2-a633-4e46-bee9-7f73cab9bae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5d263-bcc5-45fa-a196-1b4cf90f56a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_string = ' '.join(cleaned_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b144f99-b961-467f-917b-6c357a6df567",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3379bf1-96e9-40c1-bf42-147cba9426b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def read_and_normalize_text_files(root_directory):\n",
    "    file_texts = []\n",
    "    for subdir, _, files in os.walk(root_directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        text = f.read().replace('\\n', ' ')\n",
    "                        file_texts.append(text)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading file '{file_path}': {e}\")\n",
    "    return file_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1e4025-3438-4643-8a93-78454d906ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_datapath_llama3 = '/lockbox/llama3_20240509/llama3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caaf25f-2b0c-4dcd-8142-dbb7116783df",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_text_llama3 = read_and_normalize_text_files(candidate_datapath_llama3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facad31d-86dd-4e23-a099-dc86b93ad9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_text_llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e674c6-8cdd-4adf-a436-0b1c84d940e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_string_1 = ' '.join(syn_text_llama3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4652da93-68cb-4700-8d11-a54919279ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_string_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990660c3-0e5e-43d6-9d4b-bdeec76f9cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, bigrams, trigrams, FreqDist\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9de5c75-53f4-456e-9144-a5d7ff6dfb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aac1d2-7f76-4d66-9a76-8c8df40f169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_combined = preprocess(combined_string)\n",
    "tokens_annotated = preprocess(combined_string_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5e4a55-6681-41c4-9068-ba2c93c59402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_ngrams(tokens, n=10):\n",
    "    unigram_freq = FreqDist(tokens)\n",
    "    bigram_freq = FreqDist(bigrams(tokens))\n",
    "    trigram_freq = FreqDist(trigrams(tokens))\n",
    "\n",
    "    top_unigrams = unigram_freq.most_common(n)\n",
    "    top_bigrams = bigram_freq.most_common(n)\n",
    "    top_trigrams = trigram_freq.most_common(n)\n",
    "\n",
    "    return top_unigrams, top_bigrams, top_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0887c0b1-2d18-4780-b1bb-c204a98f87f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_unigrams_combined, top_bigrams_combined, top_trigrams_combined = find_top_ngrams(tokens_combined, 10)\n",
    "top_unigrams_annotated, top_bigrams_annotated, top_trigrams_annotated = find_top_ngrams(tokens_annotated, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b0d4a3-afb4-4274-8caa-57c339e23261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_ngrams(title, unigrams, bigrams, trigrams):\n",
    "    print(f\"--- {title} ---\")\n",
    "    print(\"Top 10 Unigrams:\", unigrams)\n",
    "    print(\"Top 10 Bigrams:\", bigrams)\n",
    "    print(\"Top 10 Trigrams:\", trigrams)\n",
    "    print(\"\\n\")\n",
    "\n",
    "print_top_ngrams(\"Combined String\", top_unigrams_combined, top_bigrams_combined, top_trigrams_combined)\n",
    "print_top_ngrams(\"Annotated String\", top_unigrams_annotated, top_bigrams_annotated, top_trigrams_annotated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eb848f-a004-4c4e-ab92-1856d987c2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_freqs(ngrams):\n",
    "    total = sum(freq for _, freq in ngrams)\n",
    "    return [(ngram, freq * 100 / total) for ngram, freq in ngrams]\n",
    "\n",
    "top_unigrams_combined_norm = normalize_freqs(top_unigrams_combined)\n",
    "top_bigrams_combined_norm = normalize_freqs(top_bigrams_combined)\n",
    "top_trigrams_combined_norm = normalize_freqs(top_trigrams_combined)\n",
    "\n",
    "top_unigrams_annotated_norm = normalize_freqs(top_unigrams_annotated)\n",
    "top_bigrams_annotated_norm = normalize_freqs(top_bigrams_annotated)\n",
    "top_trigrams_annotated_norm = normalize_freqs(top_trigrams_annotated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc02c13-4912-4efe-adb4-a49201c9bfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_ngram_freqs(top_unigrams, top_bigrams, top_trigrams):\n",
    "    ngrams_labels = [label for label, _ in top_unigrams] + [f\"{a} {b}\" for a, b in dict(top_bigrams).keys()] + [f\"{a} {b} {c}\" for a, b, c in dict(top_trigrams).keys()]\n",
    "    unigram_freqs = [freq for _, freq in top_unigrams]\n",
    "    bigram_freqs = [freq for _, freq in top_bigrams]\n",
    "    trigram_freqs = [freq for _, freq in top_trigrams]\n",
    "    unigram_freqs.extend([0] * (len(ngrams_labels) - len(unigram_freqs)))\n",
    "    bigram_freqs.extend([0] * (len(ngrams_labels) - len(bigram_freqs)))\n",
    "    trigram_freqs.extend([0] * (len(ngrams_labels) - len(trigram_freqs)))\n",
    "    return ngrams_labels, unigram_freqs, bigram_freqs, trigram_freqs\n",
    "\n",
    "labels_combined, unigrams_combined, bigrams_combined, trigrams_combined = aggregate_ngram_freqs(top_unigrams_combined_norm, top_bigrams_combined_norm, top_trigrams_combined_norm)\n",
    "labels_annotated, unigrams_annotated, bigrams_annotated, trigrams_annotated = aggregate_ngram_freqs(top_unigrams_annotated_norm, top_bigrams_annotated_norm, top_trigrams_annotated_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bbaa34-0192-46b4-94d5-0b455976f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42537d1d-f1bf-4cc0-bbc8-9bff485a2442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_ngrams(ngram_freq, total):\n",
    "    return [(ngram, count / total * 100) for ngram, count in ngram_freq]\n",
    "\n",
    "def plot_normalized_ngrams(unigrams, bigrams, trigrams, title, output_filename):\n",
    "    total_unigrams = sum([count for _, count in unigrams])\n",
    "    total_bigrams = sum([count for _, count in bigrams])\n",
    "    total_trigrams = sum([count for _, count in trigrams])\n",
    "\n",
    "    unigrams_norm = normalize_ngrams(unigrams, total_unigrams)\n",
    "    bigrams_norm = normalize_ngrams(bigrams, total_bigrams)\n",
    "    trigrams_norm = normalize_ngrams(trigrams, total_trigrams)\n",
    "\n",
    "    unigram_labels = [label for label, _ in unigrams_norm]\n",
    "    unigram_values = [value for _, value in unigrams_norm]\n",
    "\n",
    "    bigram_labels = [\" \".join(pair) for pair, _ in bigrams_norm]\n",
    "    bigram_values = [value for _, value in bigrams_norm]\n",
    "\n",
    "    trigram_labels = [\" \".join(trio) for trio, _ in trigrams_norm]\n",
    "    trigram_values = [value for _, value in trigrams_norm]\n",
    "\n",
    "    all_labels = unigram_labels + bigram_labels + trigram_labels\n",
    "    indices = np.arange(len(all_labels))\n",
    "\n",
    "    bar_width = 0.3\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    ax.bar(indices[:len(unigram_values)], unigram_values, width=bar_width, label=\"Unigrams\", color=\"blue\")\n",
    "    ax.bar(indices[len(unigram_values):len(unigram_values) + len(bigram_values)], bigram_values, width=bar_width, label=\"Bigrams\", color=\"green\")\n",
    "    ax.bar(indices[len(unigram_values) + len(bigram_values):], trigram_values, width=bar_width, label=\"Trigrams\", color=\"red\")\n",
    "\n",
    "    ax.set_xticks(indices)\n",
    "    ax.set_xticklabels(all_labels, rotation=45, ha=\"right\")\n",
    "\n",
    "    ax.set_ylabel('Frequency (%)')\n",
    "    ax.set_xlabel('N-grams')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(output_filename, format='png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "plot_normalized_ngrams(top_unigrams_combined, top_bigrams_combined, top_trigrams_combined, \"SGPGI Dataset Normalized N-grams\", \"SGPGIngramsplot.png\")\n",
    "plot_normalized_ngrams(top_unigrams_annotated, top_bigrams_annotated, top_trigrams_annotated, \"Synthetic SGPGI Dataset Normalized N-grams\", \"SGPGIsynngramplot.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac98ae8-1d31-4f68-8d96-d73f6da38eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_statistics(text):\n",
    "    words = text.split()\n",
    "    num_chars = len(text)\n",
    "    num_words = len(words)\n",
    "    \n",
    "    word_lengths = [len(word) for word in words]\n",
    "    mean_word_length = np.mean(word_lengths)\n",
    "    std_error_word_length = np.std(word_lengths) / np.sqrt(len(word_lengths))\n",
    "    median_word_length = np.median(word_lengths)\n",
    "    min_word_length = np.min(word_lengths)\n",
    "    max_word_length = np.max(word_lengths)\n",
    "    \n",
    "    return {\n",
    "        'Total Characters': num_chars,\n",
    "        'Total Words': num_words,\n",
    "        'Mean Word Length': mean_word_length,\n",
    "        'SE Word Length': std_error_word_length,\n",
    "        'Median Word Length': median_word_length,\n",
    "        'Min Word Length': min_word_length,\n",
    "        'Max Word Length': max_word_length\n",
    "    }\n",
    "\n",
    "combined_stats = compute_statistics(combined_string)\n",
    "annotated_stats = compute_statistics(combined_string_1)\n",
    "\n",
    "print(\"Statistics for Combined String:\")\n",
    "for key, value in combined_stats.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\nStatistics for Annotated String:\")\n",
    "for key, value in annotated_stats.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463fcdb4-87d6-4ff0-bb75-2c9238a73b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from nltk import bigrams, trigrams, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text.lower())\n",
    "    words = word_tokenize(text)\n",
    "    return [word for word in words if word not in stop_words and 'phi' not in word and 'type' not in word]\n",
    "\n",
    "def extract_context_without_phi(text, window_size=5):\n",
    "    phi_regex = re.compile(r'</?PHI[^>]*>')\n",
    "\n",
    "    phi_positions = [(m.start(), m.end()) for m in phi_regex.finditer(text)]\n",
    "\n",
    "    words = []\n",
    "    prev_end = 0\n",
    "    for start, end in phi_positions:\n",
    "        context_before = clean_and_tokenize(text[prev_end:start])\n",
    "        context_after = clean_and_tokenize(text[end:min(len(text), end + window_size * 10)])\n",
    "\n",
    "        words.extend(context_before)\n",
    "        words.extend(context_after)\n",
    "\n",
    "        prev_end = end\n",
    "\n",
    "    if prev_end < len(text):\n",
    "        words.extend(clean_and_tokenize(text[prev_end:]))\n",
    "\n",
    "    top_unigrams = Counter(words).most_common(10)\n",
    "    top_bigrams = Counter(bigrams(words)).most_common(10)\n",
    "    top_trigrams = Counter(trigrams(words)).most_common(10)\n",
    "\n",
    "    return top_unigrams, top_bigrams, top_trigrams\n",
    "\n",
    "top_unigrams_annotated, top_bigrams_annotated, top_trigrams_annotated = extract_context_without_phi(combined_string)\n",
    "top_unigrams_combined, top_bigrams_combined, top_trigrams_combined = extract_context_without_phi(combined_string_1)\n",
    "\n",
    "print(\"\\nTop 10 Surrounding Unigrams (Annotated):\")\n",
    "for unigram, count in top_unigrams_annotated:\n",
    "    print(f\"{unigram}: {count}\")\n",
    "\n",
    "print(\"\\nTop 10 Surrounding Bigrams (Annotated):\")\n",
    "for bigram, count in top_bigrams_annotated:\n",
    "    print(f\"{bigram}: {count}\")\n",
    "\n",
    "print(\"\\nTop 10 Surrounding Trigrams (Annotated):\")\n",
    "for trigram, count in top_trigrams_annotated:\n",
    "    print(f\"{trigram}: {count}\")\n",
    "\n",
    "# Print results for combined text\n",
    "print(\"\\nTop 10 Surrounding Unigrams (Combined):\")\n",
    "for unigram, count in top_unigrams_combined:\n",
    "    print(f\"{unigram}: {count}\")\n",
    "\n",
    "print(\"\\nTop 10 Surrounding Bigrams (Combined):\")\n",
    "for bigram, count in top_bigrams_combined:\n",
    "    print(f\"{bigram}: {count}\")\n",
    "\n",
    "print(\"\\nTop 10 Surrounding Trigrams (Combined):\")\n",
    "for trigram, count in top_trigrams_combined:\n",
    "    print(f\"{trigram}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8396f855-5d45-41fd-9ade-3f79e95a6e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_normalized_ngrams(unigrams, bigrams, trigrams, title, filename):\n",
    "    unigram_labels, unigram_counts = zip(*unigrams)\n",
    "    bigram_labels, bigram_counts = zip(*bigrams)\n",
    "    trigram_labels, trigram_counts = zip(*trigrams)\n",
    "\n",
    "    total_unigrams = sum(unigram_counts)\n",
    "    total_bigrams = sum(bigram_counts)\n",
    "    total_trigrams = sum(trigram_counts)\n",
    "\n",
    "    unigram_percentages = [count / total_unigrams * 100 for count in unigram_counts]\n",
    "    bigram_percentages = [count / total_bigrams * 100 for count in bigram_counts]\n",
    "    trigram_percentages = [count / total_trigrams * 100 for count in trigram_counts]\n",
    "\n",
    "    unigram_x_positions = np.arange(len(unigrams))\n",
    "    bigram_x_positions = np.arange(len(bigrams)) + len(unigrams)\n",
    "    trigram_x_positions = np.arange(len(trigrams)) + len(unigrams) + len(bigrams)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    ax.bar(unigram_x_positions, unigram_percentages, width=0.4, label='Unigrams')\n",
    "    ax.bar(bigram_x_positions, bigram_percentages, width=0.4, label='Bigrams')\n",
    "    ax.bar(trigram_x_positions, trigram_percentages, width=0.4, label='Trigrams')\n",
    "\n",
    "    all_positions = np.concatenate([unigram_x_positions, bigram_x_positions, trigram_x_positions])\n",
    "    all_labels = [label for label in unigram_labels] + [f'{a}, {b}' for a, b in bigram_labels] + [f'{a}, {b}, {c}' for a, b, c in trigram_labels]\n",
    "\n",
    "    ax.set_xticks(all_positions)\n",
    "    ax.set_xticklabels(all_labels, rotation=45, ha='right')\n",
    "\n",
    "    ax.set_xlabel('N-Grams')\n",
    "    ax.set_ylabel('Frequency (%)')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, format='png')\n",
    "    plt.show()\n",
    "\n",
    "plot_normalized_ngrams(top_unigrams_combined, top_bigrams_combined, top_trigrams_combined, \"SGPGI Dataset PHI N-Grams\", \"sgpgiPHI_ngrams.png\")\n",
    "plot_normalized_ngrams(top_unigrams_annotated, top_bigrams_annotated, top_trigrams_annotated, \"Synthetic SGPGI Dataset PHI N-Grams\", \"sgpgiSYNPHI_ngrams.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cc7821-4839-4a85-af0f-4af2c2a51410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2947527e-1874-4b81-87f0-044780e323e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_distance(set1, set2):\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    return 1 - len(intersection) / len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62991155-6da6-4912-a69e-138734f4f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_combined = set(tokens_combined)\n",
    "set_annotated = set(tokens_annotated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831adce3-e100-42bf-9d59-75ee9f13ee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = jaccard_distance(set_combined, set_annotated)\n",
    "print(f\"Jaccard Distance: {distance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bc22fe-0a7a-4cc1-8216-dcd0a318c90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/lokesh/ds_comparison/bert_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891fcd73-b7cc-4402-9733-64562a8a72e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0139d1ab-28d2-4ae7-a2dc-19b1c76ad080",
   "metadata": {},
   "outputs": [],
   "source": [
    "P, R, F1 = score([combined_string], [combined_string_1], lang='en', model_type=\"dmis-lab/biobert-v1.1\")\n",
    "print(\"Precision:\", P)\n",
    "print(\"Recall:\", R)\n",
    "print(\"F1 Score:\", F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f6246f-317c-488b-9f26-342e1bb58333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e43729e-6aa2-4a36-8b7d-6d0292bcdebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d72cc30-893f-4358-95a0-d3d488ed8cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
